{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Presto C++ Workshop","text":""},{"location":"#deploying-presto-c-and-benchmarking-with-pbench","title":"Deploying Presto C++ and Benchmarking with pbench","text":"<p>Welcome to our Presto C++ workshop! This workshop will provide an overview of Presto C++, the next-gen Presto worker. You will learn how to deploy Presto C++ with Docker Compose on your machine and query TPC-H, TPC-DS data from your local file-system with the Hive and Iceberg connectors (using file-based hive metastore). We will then demonstrate how to run queries manually with Presto CLI and look at how the open source benchmarking tool pbench can be used to run TPC-DS benchmarks against your Presto cluster. We will use pbench to compare the performance of Presto and Presto C++ on the TPC-DS benchmark.</p> <p>By the end of the workshop, you will know</p> <ul> <li>What Presto C++ is and its benefits</li> <li>How to set up and deploy Presto C++ with Docker Compose locally</li> <li>How to use the Presto CLI to run queries</li> <li>How to use pbench to run benchmarks and view results</li> <li>How to get a quantified speed-up factor of Presto C++ vs Java</li> </ul> <p>This workshop is organized into the following sections:</p> <ul> <li>Agenda</li> <li>Compatibility</li> <li>Technology Used</li> <li>Credits</li> </ul>"},{"location":"#agenda","title":"Agenda","text":"Part 1 Set-up and deployment of Presto C++ and Presto Java locally Part 2 Downloading and running pbench"},{"location":"#compatibility","title":"Compatibility","text":"<p>This workshop has been tested on the following platforms:</p> <ul> <li>aarch64: Apple Silicon (M-series chips)</li> </ul>"},{"location":"#technology-used","title":"Technology Used","text":"<ul> <li>Docker: A container engine to run several applications in self-contained containers</li> <li>Docker Compose: A YAML-based tool for defining and running multi-container applications</li> <li>Presto: Fast and Reliable SQL Engine for Data Analytics and the Open Lakehouse</li> <li>pbench: A new, open-source benchmarking tool for Presto</li> </ul>"},{"location":"#credits","title":"Credits","text":"<ul> <li>Pramod Satya</li> <li>Andrew Xie</li> <li>Allen Shen</li> </ul>"},{"location":"REFERENCES/","title":"References","text":"<ul> <li>Presto</li> <li>Presto Documentation</li> <li>Pbench Wiki</li> <li>Velox</li> </ul>"},{"location":"deploy-presto/","title":"Deploying Presto with Docker Compose","text":""},{"location":"deploy-presto/#prerequisites","title":"Prerequisites","text":"<ul> <li>Git</li> <li>Docker</li> <li>Docker Compose</li> </ul>"},{"location":"deploy-presto/#1-set-up","title":"1. Set Up","text":""},{"location":"deploy-presto/#11-pull-docker-images","title":"1.1 Pull docker images","text":"<p>Pull the Presto images first to allow time for downloading.</p> <p>For Presto Java coordinator and worker image:  <pre><code>docker pull prestodb/presto:latest\n</code></pre></p> <p>Download the right Presto C++ worker image based on your system architecture. 1. For <code>aarch64</code>: <pre><code>docker pull public.ecr.aws/oss-presto/presto-native:0.289-ubuntu-arm64\n</code></pre></p> <ol> <li>For <code>x64</code>, pick the latest image tag from ECR, or use the following tag: <pre><code>docker pull public.ecr.aws/oss-presto/presto-native:0.291-20250127081606-85259c3\n</code></pre></li> </ol>"},{"location":"deploy-presto/#12-clone-prestorials-repo","title":"1.2 Clone prestorials repo","text":"<p>Clone the <code>prestodb/prestorials</code> repository which contains the Docker Compose files used for deploying Presto. <pre><code>git clone https://github.com/prestodb/prestorials.git\n</code></pre></p> <p>Then change directory into the cloned repository. <pre><code>cd prestorials\n</code></pre></p>"},{"location":"deploy-presto/#2-using-presto-cli","title":"2. Using Presto CLI","text":"<p>To run queries against a Presto cluster, confirm that the coordinator has started and run Presto CLI from the  coordinator container: <pre><code>docker exec -it coordinator sh -c \"/opt/presto-cli &lt;ARGS&gt;\" \n</code></pre></p> <p>Arguments to <code>presto-cli</code> can be appended in place of <code>&lt;ARGS&gt;</code> in the above command. Verify that a given catalog and schema exist before trying to access its tables, to use <code>hive</code> catalog and <code>tpcds</code> schema for instance: <pre><code>SHOW schemas in hive;\nUSE hive.tpcds;\nSHOW tables;\n</code></pre></p>"},{"location":"deploy-presto/#3-set-up-tpc-h-tpc-ds-data","title":"3. Set Up TPC-H, TPC-DS Data","text":""},{"location":"deploy-presto/#31-generate-data-with-presto-java-connectors","title":"3.1 Generate data with Presto Java connectors","text":"<p>We will be using Presto Java's TPC-H and TPC-DS connectors to generate TPC-H and TPC-DS tables with scale factor of <code>1</code>. Navigate to <code>docker-compose/local-fs</code>, you can spinup a single node Presto Java cluster where the coordinator also acts  as a worker using docker compose file <code>docker-compose-single-node.yaml</code>. If your system architecture is <code>x64</code>, please  modify the <code>platform</code> to <code>linux/amd64</code> in the docker compose file <code>docker-compose-single-node.yaml</code>. Certain TPC-H and TPC-DS tables contain a large number of rows, so it is recommended to increase the container memory limit in  <code>docker-compose-single-node.yaml</code> to <code>10GB</code> from the default value of <code>2GB</code>: <pre><code>    deploy:\n      resources:\n        limits:\n          memory: 10G\n</code></pre></p> <p>Spinup a single node Presto Java cluster using: <pre><code>docker compose -v -f docker-compose-single-node.yaml up\n</code></pre></p> <p>Download the <code>sql</code> files <code>createHiveTpchTables.sql</code>, <code>createHiveTpcdsTables.sql</code>, <code>createIcebergTpchTables.sql</code>, and <code>createIcebergTpcdsTables.sql</code>, and copy them into the Presto server  container: <pre><code>docker cp ./createHiveTpchTables.sql &lt;container_id&gt;:/.\ndocker cp ./createHiveTpcdsTables.sql &lt;container_id&gt;:/.\ndocker cp ./createIcebergTpchTables.sql &lt;container_id&gt;:/.\ndocker cp ./createIcebergTpcdsTables.sql &lt;container_id&gt;:/.\n</code></pre></p> <p>Using the CTAS queries from files <code>createIcebergTpchTables.sql</code> and  <code>createIcebergTpcdsTables.sql</code>, we will create the <code>tpch</code> and <code>tpcds</code> schemas  in <code>iceberg</code> catalog and add tables in these schemas using the data generated by TPC-H and TPC-DS connectors. The data will be generated in <code>/home/iceberg_data</code> in <code>parquet</code> file format and <code>iceberg</code> table format: <pre><code>docker exec -it coordinator sh -c \"/opt/presto-cli -f ./createIcebergTpchTables.sql\"\ndocker exec -it coordinator sh -c \"/opt/presto-cli -f ./createIcebergTpcdsTables.sql\" \n</code></pre></p> <p>Similarly, to generate TPC-H and TPC-DS data in <code>parquet</code> file format and <code>hive</code> table format, use the CTAS queries from files <code>createHiveTpchTables.sql</code> and  <code>createHiveTpcdsTables.sql</code>. The data will be generated in <code>/home/hive_data</code>: <pre><code>docker exec -it coordinator sh -c \"/opt/presto-cli -f ./createHiveTpchTables.sql\"\ndocker exec -it coordinator sh -c \"/opt/presto-cli -f ./createHiveTpcdsTables.sql\" \n</code></pre></p> <p>The TPC-H and TPC-DS tables can now be queried with either <code>hive</code> or <code>iceberg</code> catalog using <code>tpch</code> or <code>tpcds</code> schema.</p>"},{"location":"deploy-presto/#32-prepare-data","title":"3.2 Prepare data","text":"<p>Copy data into directory <code>prestorials/</code> from the <code>presto_server</code> docker container: <pre><code>cd prestorials/\nmkdir data/\ndocker cp &lt;container_id&gt;:/home/hive_data ./data/.\ndocker cp &lt;container_id&gt;:/home/iceberg_data ./data/. \n</code></pre></p> <p>Alternatively, the TPC-H and TPC-DS data with <code>parquet</code> file format and <code>hive</code> table format can be downloaded from here. If you downloaded <code>data.tar</code> from this link, un-tar it and ensure the <code>data</code> directory is present in <code>prestorials/</code>. The <code>data.tar</code> file can then be deleted as it is no longer needed. </p> <p>Copy the <code>data</code> directory from <code>prestorials/</code> into <code>docker-compose/local-fs/</code> and into <code>docker-compose-native/local-fs/</code>: <pre><code>cp -r data docker-compose/local-fs/.\ncp -r data docker-compose-native/local-fs/.\n</code></pre></p>"},{"location":"deploy-presto/#3-deploying-presto","title":"3. Deploying Presto","text":"<p>Now that setup is complete, we can run the docker compose file to deploy a Presto cluster and use Presto CLI to run  queries.</p>"},{"location":"deploy-presto/#31-deploying-presto-c","title":"3.1 Deploying Presto C++","text":"<p>Change into the <code>docker-compose-native/local-fs</code> directory in <code>prestorials</code> and run the <code>docker compose</code> command to start the Presto C++ cluster. We specify the Docker Compose file with <code>-f docker-compose.yaml</code>; based on  whether your system architecture is <code>aarch64</code> or <code>x64</code>, use either the docker compose file <code>docker-compose-arm64.yaml</code>  or <code>docker-compose-amd64.yaml</code> respectively: </p> <ol> <li> <p>For <code>aarch64</code>: <pre><code>docker compose -v -f docker-compose-arm64.yaml up\n</code></pre></p> </li> <li> <p>For <code>x64</code>: <pre><code>docker compose -v -f docker-compose-amd64.yaml up\n</code></pre></p> </li> </ol> <p>You should now see the logs of the Presto coordinator and worker starting up. The cluster is ready once the Presto  coordinator's discovery service acknowledges requests from both the Presto C++ workers, wait for these logs:</p> <pre><code>coordinator  | 2024-07-25T23:48:15.077Z INFO    main    com.facebook.presto.server.PrestoServer ======== SERVER STARTED ========\nworker_2     | I0725 23:48:39.584002     8 PeriodicServiceInventoryManager.cpp:118] Announcement succeeded: HTTP 202. State: active.\nworker_1     | I0725 23:48:41.484305     8 PeriodicServiceInventoryManager.cpp:118] Announcement succeeded: HTTP 202. State: active.\n</code></pre>"},{"location":"deploy-presto/#32-deploying-presto-java","title":"3.2 Deploying Presto Java","text":"<p>If you just finished Step 3.1 and have a Presto C++ cluster running, skip to the next section on running benchmarks with  <code>pbench</code>. Return to this step after running <code>pbench</code> and obtaining TPC-DS benchmark times for Presto C++.</p> <p>Deploying Presto Java is very similar to Presto C++. We just use the Docker Compose file in <code>docker-compose/local-fs</code> directory of <code>prestorials</code>. Once again, based on whether your system architecture is <code>aarch64</code> or <code>x64</code>, use either the  docker compose file <code>docker-compose-arm64.yaml</code> or <code>docker-compose-amd64.yaml</code> respectively:</p> <ol> <li> <p>For <code>aarch64</code>: <pre><code>docker compose -v -f docker-compose-arm64.yaml up\n</code></pre></p> </li> <li> <p>For <code>x64</code>: <pre><code>docker compose -v -f docker-compose-amd64.yaml up\n</code></pre></p> </li> </ol>"},{"location":"deploy-presto/#33-iceberg-schema-evolution-and-time-travel","title":"3.3 Iceberg schema evolution and time travel","text":"<p>We can query the TPC-H and TPC-DS <code>iceberg</code> tables using Presto's <code>iceberg</code> connector. First set the catalog to <code>iceberg</code> and the schema to either <code>tpcds</code> or <code>tpch</code>. The <code>iceberg</code> connector lets you modify the schema in-place, try it out with these queries. The <code>iceberg</code> connector also  supports time travel using table snapshots, try it out with these queries.</p>"},{"location":"running-pbench/","title":"Running pbench","text":"<p>At this point, you should already have a Presto C++ or Presto Java cluster running.</p>"},{"location":"running-pbench/#1-download-pbench","title":"1. Download pbench","text":""},{"location":"running-pbench/#11-download-the-pbench-tar","title":"1.1 Download the pbench tar","text":"<p>Download the <code>pbench</code> tar for your platform below. This contains the compiled <code>pbench</code> binary and the relevant benchmark configuration files.</p> <ul> <li>MacOS (x64)</li> <li>MacOS (aarch64)</li> <li>Linux (x64)</li> <li>Linux (aarch64)</li> </ul>"},{"location":"running-pbench/#12-extract-pbench","title":"1.2 Extract pbench","text":"<p>Extract the downloaded tar.gz file and change into the created <code>pbench</code> directory in a new terminal window.</p> <pre><code>cd pbench\n</code></pre>"},{"location":"running-pbench/#2-run-pbench","title":"2. Run pbench","text":"<p>Once in the <code>pbench</code> directory, run pbench with <code>./pbench run</code> and specify the benchmark configuration files related to the run. This example uses the <code>sf1</code> scale factor and <code>ds_power</code> run flavor, which runs all 99 TPC-DS queries sequentially. Change the <code>catalog</code> to <code>hive</code> and <code>schema</code> to <code>tpcds</code> in the <code>json</code> configuration file <code>benchmarks/tpc-ds/sf1.json</code>.</p> <p>For Presto C++: <pre><code>./pbench run benchmarks/native_oss.json benchmarks/tpc-ds/sf1.json benchmarks/tpc-ds/ds_power.json\n</code></pre></p> <p>For Presto Java: <pre><code>./pbench run benchmarks/java_oss.json benchmarks/tpc-ds/sf1.json benchmarks/tpc-ds/ds_power.json\n</code></pre></p> <p>You should see logs for each query being submitted and the results, including execution time and row count.</p> <p>Supplying different json files allow you to run different benchmarks. For more information on this format, visit the pbench wiki.</p>"},{"location":"running-pbench/#troubleshooting","title":"Troubleshooting","text":"<p>If you see a permissions pop-up that prevents running <code>pbench</code> on MacOS, run the following command on the downloaded  <code>.tar.gz</code> file before extracting:</p> <p><pre><code>xattr -d com.apple.quarantine pbench_darwin_arm64.tar.gz\n</code></pre> Change this command depending on the <code>pbench</code> file you downloaded.</p>"}]}